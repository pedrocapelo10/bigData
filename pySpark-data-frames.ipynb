{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Spark DataFrames Project\n",
    "\n",
    "\n",
    "Spark documentation available at:\n",
    "https://spark.apache.org/docs/2.3.1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1- How many trips were started in each year present in the data set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Trips|\n",
      "+----+-----+\n",
      "|2013|54409|\n",
      "|2014|74753|\n",
      "|2015|64761|\n",
      "|2016|63628|\n",
      "|2017|50006|\n",
      "|2018|41567|\n",
      "|2019|32797|\n",
      "|2020| 6829|\n",
      "+----+-----+\n",
      "\n",
      "CPU times: user 129 ms, sys: 87.9 ms, total: 217 ms\n",
      "Wall time: 8.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import statistics as statis\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_Trips_151MB.csv')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 ).map( lambda line : line.split(';'))\\\n",
    "                   .map( lambda line: line[2].split('/')).map( lambda arr: Row( Year = arr[2].split(' ')[0]))\n",
    "    \n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    frequencies = logRowsDF.groupBy('Year').agg(count('Year').alias('Trips'))\n",
    "    frequencies_ordered = frequencies.sort('Year')\n",
    "    frequencies_ordered.show(10)\n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2- For each of the 24 hours of the day, how many taxi trips there were, what was their average trip miles and trip total cost? \n",
    " Non-integer values should be printed with two decimal places\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+--------+\n",
      "| hour|Trips|Avg_Miles|Avg_Cost|\n",
      "+-----+-----+---------+--------+\n",
      "|01 AM|11166|     2.46|   13.73|\n",
      "|01 PM|20181|     3.38|   15.79|\n",
      "|02 AM| 8832|     2.35|   12.71|\n",
      "|02 PM|20039|     3.56|   16.10|\n",
      "|03 AM| 6594|     2.48|   12.96|\n",
      "|03 PM|20708|     3.59|   17.38|\n",
      "|04 AM| 4604|     3.80|   15.19|\n",
      "|04 PM|21714|     3.34|   16.87|\n",
      "|05 AM| 4087|     5.94|   20.94|\n",
      "|05 PM|23639|     3.09|   15.20|\n",
      "|06 AM| 5629|     5.57|   19.93|\n",
      "|06 PM|25446|     2.94|   14.62|\n",
      "|07 AM|10145|     3.98|   17.09|\n",
      "|07 PM|25402|     2.94|   15.26|\n",
      "|08 AM|15695|     2.98|   13.96|\n",
      "|08 PM|22222|     3.18|   15.99|\n",
      "|09 AM|18248|     3.23|   14.47|\n",
      "|09 PM|19786|     3.40|   15.95|\n",
      "|10 AM|17777|     3.30|   15.34|\n",
      "|10 PM|18492|     3.11|   15.29|\n",
      "|11 AM|18622|     3.56|   16.48|\n",
      "|11 PM|16303|     3.07|   15.00|\n",
      "|12 AM|13544|     2.83|   14.45|\n",
      "|12 PM|19875|     3.38|   16.04|\n",
      "+-----+-----+---------+--------+\n",
      "\n",
      "CPU times: user 149 ms, sys: 38.6 ms, total: 188 ms\n",
      "Wall time: 9.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import statistics as statis\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_Trips_151MB.csv')\n",
    "    rows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(';') ) \\\n",
    "                        .map( lambda arr : Row(   hour = arr[2][11:13].split(\":\")[0]+\" \"+arr[2][20:].split(\":\")[0], \\\n",
    "                                                  miles = arr[5].replace(',',''),\\\n",
    "                                                  cost = arr[14].replace(',','')))\n",
    "    rowsDF = spark.createDataFrame( rows )\n",
    "    \n",
    "       \n",
    "    result=rowsDF.groupBy(col(\"hour\")).agg(count(\"hour\").alias(\"Trips\"),\\\n",
    "                                           format_number(avg('miles'), 2).alias(\"Avg_Miles\"),\\\n",
    "                                           format_number(avg('cost'), 2).alias(\"Avg_Cost\"))\n",
    "    resultOrder=result.sort(\"hour\")\n",
    "    \n",
    "    resultOrder.show(24)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- For each of the 24 hours of the day, which are the (up to) 5 most popular routes (pairspickup/dropoff regions) according to the the total number of taxi trips? Also report and the average fare (total trip cost). \n",
    "Non-integer values should be printed with two decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+---------+--------+\n",
      "|Pickup_Location               |Trips|Avg_Miles|Avg_Cost|\n",
      "+------------------------------+-----+---------+--------+\n",
      "|01 AM 17031081700 17031081700 |96   |0.46     |6.91    |\n",
      "|01 AM 17031081700 17031081800 |73   |0.54     |6.96    |\n",
      "|01 AM 17031081800 17031081800 |47   |0.42     |5.99    |\n",
      "|01 AM 17031081700 17031320100 |41   |0.78     |7.55    |\n",
      "|01 AM 17031081700 17031839100 |41   |0.79     |8.09    |\n",
      "|01 PM 17031320100 17031839100 |264  |0.76     |7.54    |\n",
      "|01 PM 17031839100 17031839100 |433  |0.54     |6.68    |\n",
      "|01 PM 17031839100 17031320100 |271  |0.78     |7.52    |\n",
      "|01 PM 17031839100 17031081700 |173  |0.59     |7.07    |\n",
      "|01 PM 17031081500 17031839100 |168  |0.94     |8.04    |\n",
      "|02 AM 17031081700 17031081800 |75   |0.44     |7.10    |\n",
      "|02 AM 17031081700 17031081700 |75   |0.43     |6.41    |\n",
      "|02 AM 17031081700 17031320100 |43   |0.78     |7.67    |\n",
      "|02 AM 17031081800 17031081800 |43   |0.72     |7.51    |\n",
      "|02 AM 17031081800 17031081700 |34   |0.44     |7.53    |\n",
      "|02 PM 17031839100 17031839100 |441  |0.66     |7.03    |\n",
      "|02 PM 17031839100 17031320100 |239  |0.72     |7.30    |\n",
      "|02 PM 17031320100 17031839100 |236  |3.14     |7.67    |\n",
      "|02 PM 17031081500 17031839100 |198  |1.05     |8.36    |\n",
      "|02 PM 17031081700 17031839100 |185  |0.63     |7.09    |\n",
      "|03 AM 17031081800 17031081800 |29   |0.50     |7.65    |\n",
      "|03 AM 17031081800 17031081700 |29   |0.64     |6.42    |\n",
      "|03 AM 17031081700 17031081800 |43   |0.67     |7.44    |\n",
      "|03 AM 17031081700 17031081700 |36   |0.82     |9.49    |\n",
      "|03 AM 17031081700 17031839100 |25   |0.64     |7.13    |\n",
      "|03 PM 17031839100 17031839100 |418  |0.68     |6.50    |\n",
      "|03 PM 17031839100 17031320100 |238  |0.71     |6.95    |\n",
      "|03 PM 17031320100 17031839100 |234  |0.84     |7.58    |\n",
      "|03 PM 17031839100 17031281900 |227  |0.67     |6.12    |\n",
      "|03 PM 17031839100 17031980000 |184  |13.73    |50.24   |\n",
      "|04 AM 17031081700 17031081700 |23   |0.89     |6.57    |\n",
      "|04 AM 17031081700 17031081800 |35   |0.39     |6.42    |\n",
      "|04 AM 17031839100 17031980000 |19   |16.84    |46.02   |\n",
      "|04 AM 17031980000 17031980000 |19   |0.92     |21.41   |\n",
      "|04 AM 17031081800 17031081800 |18   |0.27     |5.81    |\n",
      "|04 PM 17031839100 17031839100 |478  |0.58     |6.32    |\n",
      "|04 PM 17031839100 17031281900 |319  |0.61     |15.75   |\n",
      "|04 PM 17031839100 17031320100 |294  |0.67     |7.04    |\n",
      "|04 PM 17031320100 17031839100 |235  |0.84     |7.81    |\n",
      "|04 PM 17031839100 17031081500 |173  |1.02     |8.42    |\n",
      "|05 AM 17031320100 17031980000 |61   |16.47    |50.16   |\n",
      "|05 AM 17031980000 17031980000 |54   |3.38     |22.07   |\n",
      "|05 AM 17031081500 17031980000 |48   |13.39    |47.83   |\n",
      "|05 AM 17031839100 17031980000 |34   |16.85    |48.11   |\n",
      "|05 AM 17031081401 17031980000 |19   |16.97    |48.74   |\n",
      "|05 PM 17031839100 17031839100 |466  |0.58     |6.66    |\n",
      "|05 PM 17031839100 17031281900 |327  |0.58     |6.56    |\n",
      "|05 PM 17031320100 17031839100 |218  |0.74     |7.91    |\n",
      "|05 PM 17031839100 17031280100 |199  |0.67     |6.77    |\n",
      "|05 PM 17031839100 17031320100 |292  |0.69     |7.20    |\n",
      "|06 AM 17031320100 17031980000 |76   |17.25    |48.99   |\n",
      "|06 AM 17031980000 17031980000 |62   |3.12     |24.57   |\n",
      "|06 AM 17031081500 17031980000 |73   |14.59    |46.95   |\n",
      "|06 AM 17031281900 17031839100 |56   |0.69     |6.09    |\n",
      "|06 AM 17031320100 17031839100 |64   |0.88     |6.42    |\n",
      "|06 PM 17031839100 17031320100 |239  |0.91     |7.87    |\n",
      "|06 PM 17031839100 17031839100 |368  |0.50     |6.54    |\n",
      "|06 PM 17031839100 17031081700 |225  |0.77     |8.17    |\n",
      "|06 PM 17031839100 17031081500 |222  |0.99     |8.73    |\n",
      "|06 PM 17031839100 17031281900 |214  |0.52     |6.45    |\n",
      "|07 AM 17031281900 17031839100 |205  |0.68     |5.99    |\n",
      "|07 AM 17031280100 17031839100 |158  |0.67     |6.12    |\n",
      "|07 AM 17031839100 17031839100 |199  |0.53     |5.90    |\n",
      "|07 AM 17031320100 17031839100 |186  |0.87     |6.64    |\n",
      "|07 AM 17031081500 17031839100 |136  |1.17     |8.45    |\n",
      "|07 PM 17031839100 17031320100 |174  |0.83     |7.70    |\n",
      "|07 PM 17031839100 17031281900 |143  |0.60     |6.03    |\n",
      "|07 PM 17031839100 17031839100 |285  |0.71     |6.53    |\n",
      "|07 PM 17031839100 17031081700 |171  |0.94     |7.76    |\n",
      "|07 PM 17031839100 17031081500 |165  |0.93     |8.55    |\n",
      "|08 AM 17031280100 17031839100 |257  |0.64     |6.62    |\n",
      "|08 AM 17031081500 17031839100 |204  |0.94     |8.55    |\n",
      "|08 AM 17031320100 17031839100 |388  |0.78     |7.05    |\n",
      "|08 AM 17031839100 17031839100 |375  |0.61     |6.40    |\n",
      "|08 AM 17031281900 17031839100 |403  |0.64     |6.50    |\n",
      "|08 PM 17031839100 17031839100 |183  |0.49     |6.17    |\n",
      "|08 PM 17031980000 17031980000 |115  |1.86     |12.17   |\n",
      "|08 PM 17031839100 17031081700 |106  |0.69     |7.37    |\n",
      "|08 PM 17031839100 17031320100 |107  |0.77     |7.36    |\n",
      "|08 PM 17031839100 17031081403 |101  |1.03     |8.22    |\n",
      "|09 AM 17031839100 17031839100 |436  |0.56     |6.99    |\n",
      "|09 AM 17031281900 17031839100 |410  |0.55     |6.70    |\n",
      "|09 AM 17031320100 17031839100 |402  |0.72     |7.24    |\n",
      "|09 AM 17031081500 17031839100 |239  |0.93     |8.99    |\n",
      "|09 AM 17031280100 17031839100 |228  |0.56     |6.76    |\n",
      "|09 PM 17031839100 17031839100 |109  |0.79     |7.79    |\n",
      "|09 PM 17031839100 17031081700 |89   |0.72     |7.46    |\n",
      "|09 PM 17031081700 17031320100 |87   |0.76     |7.61    |\n",
      "|09 PM 17031980000 17031980000 |103  |2.60     |14.29   |\n",
      "|09 PM 17031980000 17031320100 |90   |34.29    |52.44   |\n",
      "|10 AM 17031839100 17031839100 |384  |0.61     |6.70    |\n",
      "|10 AM 17031320100 17031839100 |269  |0.80     |7.71    |\n",
      "|10 AM 17031281900 17031839100 |219  |0.63     |6.84    |\n",
      "|10 AM 17031839100 17031320100 |181  |0.72     |7.29    |\n",
      "|10 AM 17031081500 17031839100 |158  |0.81     |7.67    |\n",
      "|10 PM 17031839100 17031320100 |86   |0.81     |7.26    |\n",
      "|10 PM 17031980000 17031980000 |76   |0.88     |16.29   |\n",
      "|10 PM 17031980000 17031839100 |71   |15.13    |49.91   |\n",
      "|10 PM 17031081700 17031320100 |74   |1.11     |94.00   |\n",
      "|10 PM 17031839100 17031839100 |97   |0.65     |6.63    |\n",
      "|11 AM 17031839100 17031839100 |447  |0.66     |6.98    |\n",
      "|11 AM 17031839100 17031081700 |160  |0.83     |7.43    |\n",
      "|11 AM 17031320100 17031839100 |215  |0.85     |7.53    |\n",
      "|11 AM 17031839100 17031081500 |154  |1.02     |7.89    |\n",
      "|11 AM 17031839100 17031320100 |241  |0.76     |7.91    |\n",
      "|11 PM 17031081700 17031081700 |75   |0.50     |7.44    |\n",
      "|11 PM 17031839100 17031081700 |70   |0.71     |7.31    |\n",
      "|11 PM 17031081700 17031081800 |55   |0.63     |6.79    |\n",
      "|11 PM 17031839100 17031320100 |64   |0.69     |7.08    |\n",
      "|11 PM 17031081700 17031839100 |58   |0.63     |7.03    |\n",
      "|12 AM 17031081700 17031081700 |96   |0.37     |6.49    |\n",
      "|12 AM 17031081700 17031081800 |58   |0.56     |6.88    |\n",
      "|12 AM 17031081800 17031081700 |50   |0.37     |8.35    |\n",
      "|12 AM 17031081700 17031320100 |50   |0.73     |7.39    |\n",
      "|12 AM 17031839100 17031839100 |41   |0.62     |7.73    |\n",
      "|12 PM 17031839100 17031839100 |429  |0.81     |7.13    |\n",
      "|12 PM 17031320100 17031839100 |223  |0.73     |7.58    |\n",
      "|12 PM 17031839100 17031320100 |250  |0.76     |7.59    |\n",
      "|12 PM 17031839100 17031081700 |199  |0.69     |7.61    |\n",
      "|12 PM 17031081500 17031839100 |165  |1.05     |7.92    |\n",
      "+------------------------------+-----+---------+--------+\n",
      "\n",
      "CPU times: user 229 ms, sys: 107 ms, total: 336 ms\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import statistics as statis\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_Trips_151MB.csv')\n",
    "    rows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(';') ) \\\n",
    "                        .filter( lambda v: len(v[6])>0 and len(v[7])>0) \\\n",
    "                        .map( lambda arr : Row(    hour = arr[2][11:13].split(\":\")[0]+\" \"+arr[2][20:].split(\":\")[0]+\" \", \\\n",
    "                                                  pickup = arr[6]+\" \", dropoff = arr[7]+\" \",\\\n",
    "                                                  miles = arr[5].replace(',',''),\\\n",
    "                                                  cost = arr[14].replace(',','')))\n",
    "    df = spark.createDataFrame( rows )\n",
    "       \n",
    "    result=df.groupBy('hour','pickup','dropoff').agg(count('*').alias(\"Trips\"),\\\n",
    "                                           format_number(avg('miles'), 2).alias(\"Avg_Miles\"),\\\n",
    "                                           format_number(avg('cost'), 2).alias(\"Avg_Cost\"))\n",
    "    \n",
    "    window = Window.partitionBy(result[\"hour\"]).orderBy(result['Trips'].desc())\n",
    "    \n",
    "    #result.show()\n",
    "    resultv1=result.select('*', row_number().over(window).alias('rank')).filter(col('rank') <= 5).sort('hour').drop('rank')\n",
    "    solutionv0=resultv1.withColumn('Pickup_Location', F.concat('hour','pickup','dropoff'))\n",
    "    solutionv1=solutionv0.drop('hour','pickup','dropoff')\n",
    "    solutionv2=solutionv1.select('Pickup_Location','Trips','Avg_Miles','Avg_Cost')\n",
    "    solutionv2.show(5*24,False)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4- How many payments, and each payment type, where made in each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|pickupRegion| paymenType|count|\n",
      "+------------+-----------+-----+\n",
      "| 17031980100|Credit Card| 2919|\n",
      "| 17031980100|       Cash| 1565|\n",
      "| 17031980100|     Mobile|   15|\n",
      "| 17031980100|  No Charge|   13|\n",
      "| 17031980100|    Unknown|    5|\n",
      "| 17031980100|    Dispute|    1|\n",
      "| 17031980100|     Prcard|    1|\n",
      "| 17031980000|Credit Card|10430|\n",
      "| 17031980000|       Cash| 6010|\n",
      "| 17031980000|     Mobile|   67|\n",
      "| 17031980000|  No Charge|   39|\n",
      "| 17031980000|    Unknown|   11|\n",
      "| 17031980000|    Dispute|    9|\n",
      "| 17031980000|     Prcard|    3|\n",
      "| 17031980000|      Split|    1|\n",
      "| 17031843700|       Cash|   16|\n",
      "| 17031843700|Credit Card|    7|\n",
      "| 17031843500|       Cash|    1|\n",
      "| 17031843300|Credit Card|    2|\n",
      "| 17031843300|       Cash|    1|\n",
      "| 17031843200|       Cash|    1|\n",
      "| 17031843100|Credit Card|    2|\n",
      "| 17031843100|       Cash|    1|\n",
      "| 17031842900|Credit Card|    2|\n",
      "| 17031842900|       Cash|    2|\n",
      "| 17031842300|       Cash|  717|\n",
      "| 17031842300|Credit Card|  250|\n",
      "| 17031842300|    Unknown|    4|\n",
      "| 17031842300|  No Charge|    3|\n",
      "| 17031842300|    Dispute|    1|\n",
      "| 17031842200|       Cash| 1022|\n",
      "| 17031842200|Credit Card|  729|\n",
      "| 17031842200|  No Charge|    4|\n",
      "| 17031842200|    Unknown|    2|\n",
      "| 17031842200|     Mobile|    1|\n",
      "| 17031841900|       Cash|  602|\n",
      "| 17031841900|Credit Card|  242|\n",
      "| 17031841900|    Unknown|    3|\n",
      "| 17031841900|  No Charge|    2|\n",
      "| 17031841900|     Mobile|    1|\n",
      "| 17031841900|    Dispute|    1|\n",
      "| 17031841100|       Cash|   47|\n",
      "| 17031841100|Credit Card|   15|\n",
      "| 17031841000|Credit Card| 1076|\n",
      "| 17031841000|       Cash|  920|\n",
      "| 17031841000|  No Charge|    4|\n",
      "| 17031841000|     Mobile|    4|\n",
      "| 17031841000|    Unknown|    1|\n",
      "| 17031840300|       Cash|  156|\n",
      "| 17031840300|    Unknown|   28|\n",
      "| 17031840300|     Prcard|   18|\n",
      "| 17031840300|Credit Card|   11|\n",
      "| 17031840300|      Pcard|    8|\n",
      "| 17031839200|       Cash|   25|\n",
      "| 17031839200|Credit Card|    6|\n",
      "| 17031839100|       Cash|22105|\n",
      "| 17031839100|Credit Card|16628|\n",
      "| 17031839100|  No Charge|  117|\n",
      "| 17031839100|     Mobile|  113|\n",
      "| 17031839100|    Unknown|   40|\n",
      "| 17031839100|    Dispute|   16|\n",
      "| 17031839100|     Prcard|   15|\n",
      "| 17031839100|      Split|    2|\n",
      "| 17031839000|       Cash|  507|\n",
      "| 17031839000|Credit Card|  137|\n",
      "| 17031839000|    Unknown|    3|\n",
      "| 17031839000|  No Charge|    1|\n",
      "| 17031838300|       Cash|   82|\n",
      "| 17031838300|Credit Card|   28|\n",
      "| 17031838200|       Cash|  202|\n",
      "| 17031838200|Credit Card|   75|\n",
      "| 17031838200|    Unknown|    8|\n",
      "| 17031838200|     Prcard|    3|\n",
      "| 17031838200|     Mobile|    1|\n",
      "| 17031838100|       Cash|  259|\n",
      "| 17031838100|Credit Card|  154|\n",
      "| 17031838100|  No Charge|    2|\n",
      "| 17031838100|     Prcard|    1|\n",
      "| 17031838000|Credit Card|    7|\n",
      "| 17031838000|       Cash|    4|\n",
      "| 17031837800|       Cash|   12|\n",
      "| 17031837800|Credit Card|    5|\n",
      "| 17031837100|       Cash|   12|\n",
      "| 17031837100|    Unknown|    2|\n",
      "| 17031836900|       Cash|    1|\n",
      "| 17031836700|Credit Card|    1|\n",
      "| 17031836500|       Cash|    1|\n",
      "| 17031836200|       Cash|   42|\n",
      "| 17031836200|Credit Card|   39|\n",
      "| 17031835900|       Cash|    1|\n",
      "| 17031835200|       Cash|    1|\n",
      "| 17031834300|       Cash|    1|\n",
      "| 17031834200|       Cash|    1|\n",
      "| 17031833300|       Cash|   13|\n",
      "| 17031833300|Credit Card|    2|\n",
      "| 17031833100|       Cash|  984|\n",
      "| 17031833100|Credit Card|  356|\n",
      "| 17031833100|  No Charge|    5|\n",
      "| 17031833100|     Mobile|    3|\n",
      "| 17031833100|    Dispute|    1|\n",
      "+------------+-----------+-----+\n",
      "only showing top 100 rows\n",
      "\n",
      "CPU times: user 132 ms, sys: 63.7 ms, total: 195 ms\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import statistics as statis\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_Trips_151MB.csv')\n",
    "    rows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(';') )\\\n",
    "                        .filter(lambda v: len(v[6])>0 and len(v[15])>0) \\\n",
    "                        .map( lambda arr : Row(   pickupRegion = arr[6], \\\n",
    "                                                  paymenType=arr[15], ))\n",
    "    rowsDF = spark.createDataFrame( rows )\n",
    "    \n",
    "    table_proc=rowsDF.groupBy(col(\"pickupRegion\"),col(\"paymenType\")).count()\n",
    "       \n",
    "    resultOrder=table_proc.sort(\"pickupRegion\",\"count\",ascending=False)\n",
    "    \n",
    "    resultOrder.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
